{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Data Science Visualization with datashader\n",
    "Visualization is one of the key parts in a Data Science project. It allows us to get a global sense of our data and to understand better our results. \n",
    "There are many free and non-free tools in the market to make data visualization. One of my favourites is [datashader](https://github.com/bokeh/datashader), an open source python library that allows to visualize big amounts of data with a clean and nice API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load all libraries\n",
    "import os,sys  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader import reductions\n",
    "from datashader.colors import colormap_select, Hot, inferno\n",
    "from datashader.bokeh_ext import InteractiveImage\n",
    "from bokeh.palettes import Greens3, Blues3, Blues9\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bokeh.tile_providers import WMTSTileSource, STAMEN_TONER, STAMEN_TERRAIN\n",
    "from functools import partial\n",
    "import wget\n",
    "import zipfile\n",
    "import math\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "output_notebook()\n",
    "#print(sys.path)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with something simple. Let's draw a set of points taken from a gaussian distribution of 2 different categories. Each category will be composed by 100.000 points. The first one has a wide standard deviation, so it will be scattered, and the second one will be more compressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataset generation\n",
    "np.random.seed(1)\n",
    "num=100000\n",
    "dists = {cat: pd.DataFrame(dict(x=np.random.normal(x,std,num),\n",
    "                                y=np.random.normal(y,std,num),\n",
    "                                val=val,cat=cat))\n",
    "         for x,y,std,val,cat in \n",
    "         [(3,3,5,10,\"d1\"), (2,-5,0.2,50,\"d2\")]}\n",
    "df = pd.concat(dists,ignore_index=True)\n",
    "df[\"cat\"]=df[\"cat\"].astype(\"category\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With datashader we can control the canvas size and how to show the data. In this case we are aggregating data. You can also control the colors and the background. In this case we are going to represent the data like if you were in Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "canvas = ds.Canvas(plot_width=400, plot_height=400, x_range=(-10,10), y_range=(-10,10))\n",
    "agg = canvas.points(df,'x','y',agg=reductions.count())\n",
    "img = tf.shade(agg, cmap=Greens3, how='eq_hist')\n",
    "background = \"black\"\n",
    "img = tf.set_background(img, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of census data in the US\n",
    "We are going to see data from 300 million people living in the US. For that we are going to use the [Racial dot map](http://www.coopercenter.org/demographics/Racial-Dot-Map) dataset created by the Cooper Center, which gathered data of the population density and ethnicity makeup of the USA. The first step is to download the data as an HDF5 file. \n",
    "\n",
    "Disclaimer: Even though the data and its creators distinguish each ethnicity as \"race\", there is no scientific evidence of it. The [notion of human races is a myth](http://www.americanscientist.org/bookshelf/pub/race-finished) as it has been proved by science, it is just another excuse to make us apart. In this [post](https://miguelgfierro.com/blog/2016/how-human-intelligence-works-and-why-that-makes-us-racists/) I argue that the origin of racism is based on how human intelligence is built and how stupid we are. Therefore, I will use the word ethnicity to refer persons from different communities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_data(infile):\n",
    "    if(os.path.isfile(infile)):\n",
    "        print(\"File %s already downloaded\" % infile)\n",
    "    else:\n",
    "        url = 'http://s3.amazonaws.com/bokeh_data/census.zip'\n",
    "        wget.download(url)\n",
    "        output_path = os.path.basename(url)\n",
    "        with zipfile.ZipFile(output_path, 'r') as zipf:\n",
    "            zipf.extractall()\n",
    "        os.remove(output_path)\n",
    "census_data = '/datadrive/datashader/census.h5'\n",
    "download_data(census_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to import the data using pandas. The data consist on more than 300 million data points and each ethnicity has been encoded as a character (where 'w' is white, 'b' is black, 'a' is Asian, 'h' is Hispanic, and 'o' is other (typically Native American)).\n",
    "\n",
    "The data in the dataset uses a format called [web mercator](https://en.wikipedia.org/wiki/Web_Mercator) which is a cilindrical projection of the world coordinates. It was invented in 1569 by [Gerardus Mercator](https://en.wikipedia.org/wiki/Mercator_projection) and became the standard format for nautical purposes. The web mercator format is an adaptation of the original mercator format and it is currently used by most modern map systems such as Google Maps, Bing Maps or OpenStreetMaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_hdf('census.h5', 'census')\n",
    "df = df.rename(columns = {'race':'ethnic_group'})\n",
    "df.ethnic_group = df.ethnic_group.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to prepare the canvas and plot our first map. This map shows all the points in the dataset with the Hot color of datashader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "USA = ((-13884029,  -7453304), (2698291, 6455972))\n",
    "plot_width  = int(900)\n",
    "plot_height = int(plot_width*7.0/12)\n",
    "cvs = ds.Canvas(plot_width, plot_height, *USA)\n",
    "agg = cvs.points(df, 'meterswest', 'metersnorth')\n",
    "cm = partial(colormap_select, reverse=(background!=\"black\"))\n",
    "img = tf.shade(agg, cmap = cm(Hot,0.2), how='eq_hist')\n",
    "img = tf.set_background(img, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Champions League matches\n",
    "We can easily create a visualization of the Champion League matches from 1955 to 2016 using datashader. For that we need a dataset of the matches, such as [this one](https://github.com/jalapic/engsoccerdata/blob/master/data-raw/champs.csv) and the coordinates of the Stadiums of the teams, that you can find [here](http://opisthokonta.net/?cat=34). \n",
    "\n",
    "The first step is to treat the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_stadium = pd.read_csv(\"stadiums.csv\", usecols=['Team','Stadium','Latitude','Longitude'])\n",
    "print(\"Number of rows: %d\" % df_stadium.shape[0])\n",
    "dd1 = df_stadium.take([0,99, 64, 121])\n",
    "dd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to match the club names in the dataset of coordinates with the those in the dataset of matches. They are similar but not always exactly the same, for example, in the dataset of coordinates we have `Real Madrid FC` and in the dataset of matches we have `Real Madrid`. Furthermore, in the first one there are several entries for some teams, like `Atletico Madrid`, `Atletico Madrid B` or `Atletico Madrid C` meaning they are the teams from the first division and from other divisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_match = pd.read_csv('champions.csv', usecols=['Date','home','visitor'])\n",
    "df_teams_champions = pd.concat([df_match['home'], df_match['visitor']])\n",
    "teams_champions = set(df_teams_champions)\n",
    "print(\"Number of teams that have participated in the Champions League: %d\" % len(teams_champions))\n",
    "print(\"Number of matches in the dataset: %d\" % df_match.shape[0])\n",
    "df_match.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the string similarity you can use different methods. Here we will use a simple method to calculate it with `difflib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def get_info_similar_team(team, df_stadium, threshold=0.6, verbose=False):\n",
    "    max_rank = 0\n",
    "    max_idx = -1\n",
    "    stadium = \"Unknown\"\n",
    "    latitude = np.NaN\n",
    "    longitude = np.NaN\n",
    "    for idx, val in enumerate(df_stadium['Team']):\n",
    "        rank = similar(team, val)\n",
    "        if rank > threshold:\n",
    "            if(verbose): print(\"%s and %s(Idx=%d) are %f similar.\" % (team, val, idx, rank))\n",
    "            if rank > max_rank:\n",
    "                if(verbose): print(\"New maximum rank: %f\" %rank)\n",
    "                max_rank = rank\n",
    "                max_idx = idx\n",
    "                stadium = df_stadium['Stadium'].iloc[max_idx]\n",
    "                latitude = df_stadium['Latitude'].iloc[max_idx]\n",
    "                longitude = df_stadium['Longitude'].iloc[max_idx]\n",
    "    return stadium, latitude, longitude\n",
    "print(get_info_similar_team(\"Real Madrid FC\", df_stadium, verbose=True))\n",
    "print(get_info_similar_team(\"Atletico de Madrid FC\", df_stadium, verbose=True))\n",
    "print(get_info_similar_team(\"Inter Milan\", df_stadium, verbose=True))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a dataframe relating each match with the stadium coordinates of each team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_match_stadium = df_match\n",
    "home_stadium_index = df_match_stadium['home'].map(lambda x: get_info_similar_team(x, df_stadium))\n",
    "visitor_stadium_index = df_match_stadium['visitor'].map(lambda x: get_info_similar_team(x, df_stadium))\n",
    "df_home = pd.DataFrame(home_stadium_index.tolist(), columns=['home_stadium', 'home_latitude', 'home_longitude'])\n",
    "df_visitor = pd.DataFrame(visitor_stadium_index.tolist(), columns=['visitor_stadium', 'visitor_latitude', 'visitor_longitude'])\n",
    "df_match_stadium = pd.concat([df_match_stadium, df_home, df_visitor], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of missing values: %d out of %d\" % (df_match_stadium['home_stadium'].value_counts()['Unknown'], df_match_stadium.shape[0]))\n",
    "df1 = df_match_stadium['home_stadium'] == 'Unknown'\n",
    "df2 = df_match_stadium['visitor_stadium'] == 'Unknown'\n",
    "n_complete_matches = df_match_stadium.shape[0] - df_match_stadium[df1 | df2].shape[0]\n",
    "print(\"Number of matches with complete data: %d\" % n_complete_matches)\n",
    "df_match_stadium.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, even though there are many entries in the dataset that don't have any value, we are going to create a dataframe with the teams that do have values and advance in the project. This dataframe finds the combination of teams (home and visitor) that have values and concatenate each other to create the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aggregate_dataframe_coordinates(dataframe):\n",
    "    df = pd.DataFrame(index=np.arange(0, n_complete_matches*3), columns=['Latitude','Longitude'])\n",
    "    count = 0\n",
    "    for ii in range(dataframe.shape[0]):\n",
    "        if dataframe['home_stadium'].loc[ii]!= 'Unknown' and dataframe['visitor_stadium'].loc[ii]!= 'Unknown':\n",
    "            df.loc[count] = [dataframe['home_latitude'].loc[ii], dataframe['home_longitude'].loc[ii]]\n",
    "            df.loc[count+1] = [dataframe['visitor_latitude'].loc[ii], dataframe['visitor_longitude'].loc[ii]]\n",
    "            df.loc[count+2] = [np.NaN, np.NaN]\n",
    "            count += 3\n",
    "    return df\n",
    "df_agg = aggregate_dataframe_coordinates(df_match_stadium)\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to transform the latitude and longitude coordinates to web mercator format in order to be able to represent it in a map using bokeh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_web_mercator(yLat, xLon):\n",
    "    # Check if coordinate out of range for Latitude/Longitude\n",
    "    if (abs(xLon) > 180) and (abs(yLat) > 90):  \n",
    "        return\n",
    " \n",
    "    semimajorAxis = 6378137.0  # WGS84 spheriod semimajor axis\n",
    "    east = xLon * 0.017453292519943295\n",
    "    north = yLat * 0.017453292519943295\n",
    " \n",
    "    northing = 3189068.5 * math.log((1.0 + math.sin(north)) / (1.0 - math.sin(north)))\n",
    "    easting = semimajorAxis * east\n",
    " \n",
    "    return [easting, northing]\n",
    "df_agg_mercator = df_agg.apply(lambda row: to_web_mercator(row['Latitude'], row['Longitude']), axis=1)\n",
    "df_agg_mercator.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to plot the trayectories in the map using datashader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_width  = 850\n",
    "plot_height = 600\n",
    "x_range = (-2.0e6, 2.5e6)\n",
    "y_range = (4.1e6, 7.8e6)\n",
    "def create_image(x_range=x_range, y_range=y_range, w=plot_width, h=plot_height):\n",
    "    cvs = ds.Canvas(plot_width=w, plot_height=h, x_range=x_range, y_range=y_range)\n",
    "    agg = cvs.line(df_agg_mercator, 'Latitude', 'Longitude',  ds.count())\n",
    "    img = tf.shade(agg, cmap=Blues9, how='eq_hist')\n",
    "        \n",
    "    return img\n",
    "\n",
    "def base_plot(tools='pan,wheel_zoom,reset',plot_width=plot_width, plot_height=plot_height,**plot_args):\n",
    "    p = figure(tools=tools, plot_width=plot_width, plot_height=plot_height,\n",
    "        x_range=x_range, y_range=y_range, outline_line_color=None,\n",
    "        min_border=0, min_border_left=0, min_border_right=0,\n",
    "        min_border_top=0, min_border_bottom=0, **plot_args)\n",
    "    \n",
    "    p.axis.visible = False\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    \n",
    "    return p\n",
    "\n",
    "#img = create_image(dd1)\n",
    "ArcGIS=WMTSTileSource(url='http://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{Z}/{Y}/{X}.png')\n",
    "p = base_plot()\n",
    "p.add_tile(ArcGIS)\n",
    "InteractiveImage(p, create_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that we have the map we can start to improve it. If you are into football, you will notice that there are several points in the north of Spain, that corresponds to Sporting de Gijon. Sadly for Sporting supporters, they have never reached to the Champions. Instead, Sporting Clube de Portugal has participated several times in the championship, but since the current dataset doesn't have teams from Portugal, the system mistakenly thinks that `Sporting CP` from `champions.csv` is the Sporting de Gijon from `stadiums.csv`. So lets fix this issue by getting the stadiums coordinates from the rest of the countries in Europe.  \n",
    "We can get that info from wikidata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stadium_read = pd.read_csv('stadiums_wikidata.csv', usecols=['clubLabel','venueLabel','coordinates'])\n",
    "df_stadium_read.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to clean the column coordinates. For that we will use a regex pattern. The pattern `[-+]?[0-9]*\\.?[0-9]+` finds any signed float in a string. Then we create two patterns separated by a space and name the columns using this format: `(?P<Longitude>)`. Finally we have to concatente the club name with the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_temp = df_stadium_read['coordinates'].str.extract('(?P<Longitude>[-+]?[0-9]*\\.?[0-9]+) (?P<Latitude>[-+]?[0-9]*\\.?[0-9]+)', expand=True)\n",
    "df_stadium_new = pd.concat([df_stadium_read['clubLabel'],df_stadium_read['venueLabel'], df_temp], axis=1) \n",
    "print(\"Number of rows: %d\" % df_stadium_new.shape[0])\n",
    "unique_teams_stadium = list(set(df_stadium['clubLabel']))\n",
    "print(\"Unique team's name number: %d\" % len(unique_teams_stadium))\n",
    "df_stadium_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: Part of the code in this notebook has been taken from the example folder of [datashader](https://github.com/bokeh/datashader/tree/master/examples)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
