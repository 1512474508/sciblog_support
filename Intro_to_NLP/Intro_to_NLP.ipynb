{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing with fastText\n",
    "In this notebook we will discuss what is Natural Language Processing (NLP) and how to easily implement several projects using the library [fastText](https://github.com/facebookresearch/fastText). fastText is implemented in C++, however, there is a python wrapper, [fastText.py](https://github.com/salestock/fastText.py), that we are going to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "#Load all libraries\n",
    "import os,sys  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "try:\n",
    "    from html import unescape  # python 3.4+\n",
    "except ImportError:\n",
    "    try:\n",
    "        from html.parser import HTMLParser  # python 3.x (<3.4)\n",
    "    except ImportError:\n",
    "        from HTMLParser import HTMLParser  # python 2.x\n",
    "    unescape = HTMLParser().unescape\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "The first task will be to perform text classification dataset DBPedia, which can be accessed [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). The dataset consists of text descriptions of 14 different classes. The training set contains 560000 reviews and the test contains 70000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                               name  \\\n",
       "0      1                   E. D. Abbott Ltd   \n",
       "1      1                     Schwan-Stabilo   \n",
       "2      1                         Q-workshop   \n",
       "3      1  Marvell Software Solutions Israel   \n",
       "4      1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                         description class_name  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...    Company  \n",
       "1   Schwan-STABILO is a German maker of pens for ...    Company  \n",
       "2   Q-workshop is a Polish company located in Poz...    Company  \n",
       "3   Marvell Software Solutions Israel known as RA...    Company  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...    Company  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load train set\n",
    "train_file = 'dbpedia_train.csv'\n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Load test set\n",
    "test_file = 'dbpedia_test.csv'\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Mapping from class number to class name\n",
    "class_dict={\n",
    "1:'Company',\n",
    "2:'EducationalInstitution',\n",
    "3:'Artist',\n",
    "4:'Athlete',\n",
    "5:'OfficeHolder',\n",
    "6:'MeanOfTransportation',\n",
    "7:'Building',\n",
    "8:'NaturalPlace',\n",
    "9:'Village',\n",
    "10:'Animal',\n",
    "11:'Plant',\n",
    "12:'Album',\n",
    "13:'Film',\n",
    "14:'WrittenWork'\n",
    "}\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">3</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">12</th>\n",
       "      <th colspan=\"4\" halign=\"left\">13</th>\n",
       "      <th colspan=\"4\" halign=\"left\">14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>...</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_name</th>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>Company</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>EducationalInstitution</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Album</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>Film</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>1</td>\n",
       "      <td>WrittenWork</td>\n",
       "      <td>40000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>40000</td>\n",
       "      <td>39996</td>\n",
       "      <td>MegaPath Corporation—headquartered in Pleasan...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>39992</td>\n",
       "      <td>Lakshmi Narain College of Technology (LNCT) i...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>Before Smile Empty Soul became Smile Empty So...</td>\n",
       "      <td>2</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Hamlet is a 1948 British film adaptation of W...</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>39984</td>\n",
       "      <td>Tom Clancy's Net Force Explorers or Net Force...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Lohmann (company)</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Lakeland Community College</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>...</td>\n",
       "      <td>We Deserve Much Worse</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>Babul (1950 film)</td>\n",
       "      <td>1</td>\n",
       "      <td>40000</td>\n",
       "      <td>40000</td>\n",
       "      <td>The Supernaturalist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "class           1                                                             \\\n",
       "             count unique                                                top   \n",
       "class_name   40000      1                                            Company   \n",
       "description  40000  39996   MegaPath Corporation—headquartered in Pleasan...   \n",
       "name         40000  40000                                  Lohmann (company)   \n",
       "\n",
       "class                  2          \\\n",
       "              freq  count unique   \n",
       "class_name   40000  40000      1   \n",
       "description      2  40000  39992   \n",
       "name             1  40000  40000   \n",
       "\n",
       "class                                                                     3   \\\n",
       "                                                           top   freq  count   \n",
       "class_name                              EducationalInstitution  40000  40000   \n",
       "description   Lakshmi Narain College of Technology (LNCT) i...      2  40000   \n",
       "name                                Lakeland Community College      1  40000   \n",
       "\n",
       "class               ...                                                   12  \\\n",
       "            unique  ...                                                  top   \n",
       "class_name       1  ...                                                Album   \n",
       "description  40000  ...     Before Smile Empty Soul became Smile Empty So...   \n",
       "name         40000  ...                                We Deserve Much Worse   \n",
       "\n",
       "class                  13         \\\n",
       "              freq  count unique   \n",
       "class_name   40000  40000      1   \n",
       "description      2  40000  40000   \n",
       "name             1  40000  40000   \n",
       "\n",
       "class                                                                     14  \\\n",
       "                                                           top   freq  count   \n",
       "class_name                                                Film  40000  40000   \n",
       "description   Hamlet is a 1948 British film adaptation of W...      1  40000   \n",
       "name                                         Babul (1950 film)      1  40000   \n",
       "\n",
       "class                                                                         \n",
       "            unique                                                top   freq  \n",
       "class_name       1                                        WrittenWork  40000  \n",
       "description  39984   Tom Clancy's Net Force Explorers or Net Force...     15  \n",
       "name         40000                                The Supernaturalist      1  \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.describe().transpose()\n",
    "desc = df.groupby('class')\n",
    "desc.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to treat the data. As of today, the python wrapper of fastText doesn't allow dataframes or iterators as inputs to their functions (however, they are [working on it](https://github.com/salestock/fastText.py/issues/78). We have to create an intermediate file. This intermediate file doesn't have commas, non-ascii characters and everything is lowercase. The changes are based on [this script](https://github.com/facebookresearch/fastText/blob/a88344f6de234bdefd003e9e55512eceedde3ec0/classification-example.sh#L17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataset(dataframe, shuffle=False, encode_ascii=False, clean_strings = False, label_prefix='__label__'):\n",
    "    # Transform train file\n",
    "    df = dataframe[['name','description']].apply(lambda x: x.str.replace(',',' '))\n",
    "    df['class'] = label_prefix + dataframe['class'].astype(str) + ' '\n",
    "    if(clean_strings):\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\"',''))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\\'',' \\' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('.',' . '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('(',' ( '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(')',' ) '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('!',' ! '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('?',' ? '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(':',' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(';',' '))\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.lower())\n",
    "    if(shuffle):\n",
    "        from sklearn.utils import shuffle\n",
    "        df = shuffle(df).reset_index(drop=True)\n",
    "        #df.sample(frac=1).reset_index(drop=True)\n",
    "    if(encode_ascii):\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8'))\n",
    "    df['name'] = ' ' + df['name'] + ' '\n",
    "    df['description'] = ' ' + df['description'] + ' '\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.83 s, sys: 400 ms, total: 10.2 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Transform datasets\n",
    "df_train_clean = clean_dataset(df, True, False)\n",
    "df_test_clean = clean_dataset(df_test, False, False)\n",
    "\n",
    "# Write files to disk\n",
    "train_file_clean = 'dbpedia.train'\n",
    "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file_clean = 'dbpedia.test'\n",
    "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is cleaned, the next step is to train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 2 s, total: 1min 28s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train a classifier\n",
    "output_file = 'dp_model'\n",
    "classifier = fasttext.supervised(train_file_clean, output_file, label_prefix='__label__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can test its accuracy. We can obtain the [percision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the model. High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.9836\n",
      "R@1: 0.9836\n",
      "Number of examples: 70000\n",
      "CPU times: user 568 ms, sys: 0 ns, total: 568 ms\n",
      "Wall time: 568 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate classifier\n",
    "result = classifier.test(test_file_clean)\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to check how the model works with real sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.\n",
      "Label: 3; label name: Artist\n",
      "Sentence:  One of my favourite tennis players in the world is Rafa Nadal.\n",
      "Label: 4; label name: Athlete; certainty: 0.931641\n",
      "Sentence:  Say what one more time, I dare you, I double-dare you motherfucker!\n",
      "Label: 12; label name: Album; certainty: 0.554687\n",
      "Label: 14; label name: WrittenWork; certainty: 0.263672\n",
      "Label: 7; label name: Building; certainty: 0.056641\n"
     ]
    }
   ],
   "source": [
    "sentence1 = ['Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.']\n",
    "labels1 = classifier.predict(sentence1)\n",
    "class1 = int(labels1[0][0])\n",
    "print(\"Sentence: \", sentence1[0])\n",
    "print(\"Label: %d; label name: %s\" %(class1, class_dict[class1]))\n",
    "\n",
    "sentence2 = ['One of my favourite tennis players in the world is Rafa Nadal.']\n",
    "labels2 = classifier.predict_proba(sentence2)\n",
    "class2, prob2 = labels2[0][0] # it returns class2 as string\n",
    "print(\"Sentence: \", sentence2[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))\n",
    "\n",
    "sentence3 = ['Say what one more time, I dare you, I double-dare you motherfucker!']\n",
    "number_responses = 3\n",
    "labels3 = classifier.predict_proba(sentence3, k=number_responses)\n",
    "print(\"Sentence: \", sentence3[0])\n",
    "for l in range(number_responses):\n",
    "    class3, prob3 = labels3[0][l]\n",
    "    print(\"Label: %s; label name: %s; certainty: %f\" %(class3, class_dict[int(class3)], prob3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts sentence 1 as `Artist`, which is correct. Sentence 2 is also predicted correctly. This time we used the function `predict_proba` that retruns the certainty of the prediction as a probability. Finally, sentence 3 was not correctly classified. The correct label would be `Film`, since the sentence is from famous scene of a very good film. If by any chance, you don't know [what I'm talking about](https://www.youtube.com/watch?v=xwT60UbOZnI), well, please put your priorities in order. Stop reading this notebook, go to see Pulp Fiction, and then come back to keep learning NLP :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sentiment Analysis\n",
    "Sentiment analysis is one of the most important use cases in text classification. The objective is to classify a piece of text into positive, negative and, in some cases, neutral. This is extensively used by brands to understand the perception their customers. Sentiment analysis can influence marketing campaigns, generate leads, plan product development or improeve customer service.  \n",
    "\n",
    "In our notebook, we will use the Amazon polarity dataset, which contains 3.6 million reviews in the train set and 400.000 reviews in the test set. The reviews are positive, 1, or negative, 2. The dataset can be found [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). \n",
    "\n",
    "The first step is to prepare the dataset for the algorithm format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 5.48 s, total: 1min 43s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load train set\n",
    "train_file = 'amazon_review_polarity_train.csv'\n",
    "df_sentiment_train = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Load test set\n",
    "test_file = 'amazon_review_polarity_test.csv'\n",
    "df_sentiment_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "\n",
    "# Transform datasets\n",
    "df_train_clean = clean_dataset(df_sentiment_train, True, False)\n",
    "df_test_clean = clean_dataset(df_sentiment_test, False, False)\n",
    "\n",
    "# Write files to disk\n",
    "train_file_clean = 'amazon.train'\n",
    "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file_clean = 'amazon.test'\n",
    "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data, let's train the classifier. This time instead of the default parameters we are going to use those used in the [fastText paper](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 31s, sys: 13.1 s, total: 9min 44s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parameters\n",
    "dim=10\n",
    "lr=0.1\n",
    "epoch=5\n",
    "min_count=1\n",
    "word_ngrams=2\n",
    "bucket=10000000\n",
    "thread=12\n",
    "label_prefix='__label__'\n",
    "\n",
    "# Train a classifier\n",
    "output_file = 'amazon_model'\n",
    "classifier = fasttext.supervised(train_file_clean, output_file, dim=dim, lr=lr, epoch=epoch,\n",
    "                                 min_count=min_count, word_ngrams=word_ngrams, bucket=bucket,\n",
    "                                 thread=thread, label_prefix=label_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.941605\n",
      "R@1: 0.941605\n",
      "Number of examples: 400000\n",
      "CPU times: user 5.73 s, sys: 264 ms, total: 5.99 s\n",
      "Wall time: 5.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate classifier\n",
    "result = classifier.test(test_file_clean)\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The product design is nice but it's working as expected\n",
      "Label: 2; label name: Positive; certainty: 0.986328\n",
      "Sentence:  I bought the product a month ago and it was working correctly. But now is not working great\n",
      "Label: 1; label name: Negative; certainty: 0.871094\n"
     ]
    }
   ],
   "source": [
    "class_dict={\n",
    "    1:\"Negative\",\n",
    "    2:\"Positive\"\n",
    "}\n",
    "\n",
    "sentence1 = [\"The product design is nice but it's working as expected\"]\n",
    "labels1 = classifier.predict_proba(sentence1)\n",
    "class1, prob1 = labels1[0][0] # it returns class as string\n",
    "print(\"Sentence: \", sentence1[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class1, class_dict[int(class1)], prob1))\n",
    "\n",
    "sentence2 = [\"I bought the product a month ago and it was working correctly. But now is not working great\"]\n",
    "labels2 = classifier.predict_proba(sentence2)\n",
    "class2, prob2 = labels2[0][0] # it returns class as string\n",
    "print(\"Sentence: \", sentence2[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a tweet as the text input. Twitter has a [REST API](https://dev.twitter.com/rest/public) that allows you to interact with all their data. However, everything from that API requires authentication (an example can be found in this [Twitter bot](https://github.com/miguelgfierro/twitter_bot) I made). Instead, we can parse the title of the tweet url, which contains the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miguel G-Fierro on Twitter: &quot;Today at @Microsoft we had a demo of the hololens. It is one of the most spectacular things I&#39;ve seen in my life&quot;\n",
      "Miguel G-Fierro on Twitter: \"Today at @Microsoft we had a demo of the hololens. It is one of the most spectacular things I've seen in my life\"\n",
      "Label: 2; label name: Positive; certainty: 0.998047\n"
     ]
    }
   ],
   "source": [
    "# Get twitter data\n",
    "url = \"https://twitter.com/miguelgfierro/status/805827479139192832\"\n",
    "response = urlopen(url).read()\n",
    "title = str(response).split('<title>')[1].split('</title>')[0]\n",
    "print(title)\n",
    "\n",
    "# Format tweet\n",
    "tweet = unescape(title)\n",
    "print(tweet)\n",
    "\n",
    "# Classify tweet\n",
    "label_tweet = classifier.predict_proba([tweet])\n",
    "class_tweet, prob_tweet = label_tweet[0][0]\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class_tweet, class_dict[int(class_tweet)], prob_tweet))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
