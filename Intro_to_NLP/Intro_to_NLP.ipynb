{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing with fastText\n",
    "In this notebook we will discuss what is Natural Language Processing (NLP) and how to easily implement several projects using the library [fastText](https://github.com/facebookresearch/fastText). fastText is implemented in C++, however, there is a python wrapper, [fastText.py](https://github.com/salestock/fastText.py), that we are going to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"
     ]
    }
   ],
   "source": [
    "#Load all libraries\n",
    "import os,sys  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "The first task will be to perform text classification dataset DBPedia, which can be accessed [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). The dataset consists of text descriptions of 14 different classes. The training set contains 560000 reviews and the test contains 70000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load train set\n",
    "train_file = 'dbpedia_train.csv'\n",
    "df = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Load test set\n",
    "test_file = 'dbpedia_test.csv'\n",
    "df_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Mapping from class number to class name\n",
    "class_dict={\n",
    "1:'Company',\n",
    "2:'EducationalInstitution',\n",
    "3:'Artist',\n",
    "4:'Athlete',\n",
    "5:'OfficeHolder',\n",
    "6:'MeanOfTransportation',\n",
    "7:'Building',\n",
    "8:'NaturalPlace',\n",
    "9:'Village',\n",
    "10:'Animal',\n",
    "11:'Plant',\n",
    "12:'Album',\n",
    "13:'Film',\n",
    "14:'WrittenWork'\n",
    "}\n",
    "df['class_name'] = df['class'].map(class_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df.describe().transpose()\n",
    "desc = df.groupby('class')\n",
    "desc.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to treat the data. As of today, the python wrapper of fastText doesn't allow dataframes or iterators as inputs to their functions (however, they are [working on it](https://github.com/salestock/fastText.py/issues/78). We have to create an intermediate file. This intermediate file doesn't have commas, non-ascii characters and everything is lowercase. The changes are based on [this script](https://github.com/facebookresearch/fastText/blob/a88344f6de234bdefd003e9e55512eceedde3ec0/classification-example.sh#L17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataset(dataframe, shuffle=False, encode=False, label_prefix='__label__'):\n",
    "    # Transform train file\n",
    "    df = dataframe[['name','description']].apply(lambda x: x.str.replace(',',' '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\"',''))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('\\'',' \\' '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('.',' . '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('(',' ( '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(')',' ) '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('!',' ! '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace('?',' ? '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(':',' '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.replace(';',' '))\n",
    "    df[['name','description']] = df[['name','description']].apply(lambda x: x.str.lower())\n",
    "    df['class'] = label_prefix + dataframe['class'].astype(str) + ' '\n",
    "    if(shuffle):\n",
    "        from sklearn.utils import shuffle\n",
    "        df = shuffle(df).reset_index(drop=True)\n",
    "        #df.sample(frac=1).reset_index(drop=True)\n",
    "    if(encode):\n",
    "        df[['name','description']] = df[['name','description']].apply(lambda x: x.str.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8'))\n",
    "    df['name'] = ' ' + df['name'] + ' '\n",
    "    df['description'] = ' ' + df['description'] + ' '\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Transform datasets\n",
    "df_train_clean = clean_dataset(df, True, False)\n",
    "df_test_clean = clean_dataset(df_test, False, False)\n",
    "\n",
    "# Write files to disk\n",
    "train_file_clean = 'dbpedia.train'\n",
    "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file_clean = 'dbpedia.test'\n",
    "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is cleaned, the next step is to train the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train a classifier\n",
    "output_file = 'dp_model'\n",
    "classifier = fasttext.supervised(train_file_clean, output_file, label_prefix='__label__')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can test its accuracy. We can obtain the [percision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the model. High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate classifier\n",
    "result = classifier.test(test_file_clean)\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to check how the model works with real sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence1 = ['Picasso was a famous painter born in Malaga, Spain. He revolutionized the art in the 20th century.']\n",
    "labels1 = classifier.predict(sentence1)\n",
    "class1 = int(labels1[0][0])\n",
    "print(\"Sentence: \", sentence1[0])\n",
    "print(\"Label: %d; label name: %s\" %(class1, class_dict[class1]))\n",
    "\n",
    "sentence2 = ['One of my favourite tennis players in the world is Rafa Nadal.']\n",
    "labels2 = classifier.predict_proba(sentence2)\n",
    "class2, prob2 = labels2[0][0] # it returns class2 as string\n",
    "print(\"Sentence: \", sentence2[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))\n",
    "\n",
    "sentence3 = ['Say what one more time, I dare you, I double-dare you motherfucker!']\n",
    "number_responses = 3\n",
    "labels3 = classifier.predict_proba(sentence3, k=number_responses)\n",
    "print(\"Sentence: \", sentence3[0])\n",
    "for l in range(number_responses):\n",
    "    class3, prob3 = labels3[0][l]\n",
    "    print(\"Label: %s; label name: %s; certainty: %f\" %(class3, class_dict[int(class3)], prob3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts sentence 1 as `Artist`, which is correct. Sentence 2 is also predicted correctly. This time we used the function `predict_proba` that retruns the certainty of the prediction as a probability. Finally, sentence 3 was not correctly classified. The correct label would be `Film`, since the sentence is from famous scene of a very good film. If by any chance, you don't know [what I'm talking about](https://www.youtube.com/watch?v=xwT60UbOZnI), well, please put your priorities in order. Stop reading this notebook, go to see Pulp Fiction, and then come back to keep learning NLP :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sentiment Analysis\n",
    "Sentiment analysis is one of the most important use cases in text classification. The objective is to classify a piece of text into positive, negative and, in some cases, neutral. This is extensively used by brands to understand the perception their customers. Sentiment analysis can influence marketing campaigns, generate leads, plan product development or improeve customer service.  \n",
    "\n",
    "In our notebook, we will use the Amazon polarity dataset, which contains 3.6 million reviews in the train set and 400.000 reviews in the test set. The reviews are positive, 1, or negative, 2. The dataset can be found [here](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M). \n",
    "\n",
    "The first step is to prepare the dataset for the algorithm format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Load train set\n",
    "train_file = 'amazon_review_polarity_train.csv'\n",
    "df_sentiment_train = pd.read_csv(train_file, header=None, names=['class','name','description'])\n",
    "\n",
    "#Load test set\n",
    "test_file = 'amazon_review_polarity_test.csv'\n",
    "df_sentiment_test = pd.read_csv(test_file, header=None, names=['class','name','description'])\n",
    "\n",
    "# Transform datasets\n",
    "df_train_clean = clean_dataset(df_sentiment_train, True, False)\n",
    "df_test_clean = clean_dataset(df_sentiment_test, False, False)\n",
    "\n",
    "# Write files to disk\n",
    "train_file_clean = 'amazon.train'\n",
    "df_train_clean.to_csv(train_file_clean, header=None, index=False, columns=['class','name','description'] )\n",
    "\n",
    "test_file_clean = 'amazon.test'\n",
    "df_test_clean.to_csv(test_file_clean, header=None, index=False, columns=['class','name','description'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data, let's train the classifier. This time instead of the default parameters we are going to use those used in the [fastText paper](https://arxiv.org/abs/1607.01759)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 28s, sys: 6.72 s, total: 8min 34s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parameters\n",
    "dim=10\n",
    "lr=0.1\n",
    "epoch=5\n",
    "min_count=1\n",
    "word_ngrams=2\n",
    "bucket=10000000\n",
    "thread=12\n",
    "label_prefix='__label__'\n",
    "\n",
    "# Train a classifier\n",
    "output_file = 'amazon_model'\n",
    "classifier = fasttext.supervised(train_file_clean, output_file, dim=dim, lr=lr, epoch=epoch,\n",
    "                                 min_count=min_count, word_ngrams=word_ngrams, bucket=bucket,\n",
    "                                 thread=thread, label_prefix=label_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@1: 0.9466975\n",
      "R@1: 0.9466975\n",
      "Number of examples: 400000\n",
      "CPU times: user 4.41 s, sys: 76 ms, total: 4.48 s\n",
      "Wall time: 4.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Evaluate classifier\n",
    "result = classifier.test(test_file_clean)\n",
    "print('P@1:', result.precision)\n",
    "print('R@1:', result.recall)\n",
    "print ('Number of examples:', result.nexamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's evaluate some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  I hate when this situations happen\n",
      "Label: 2; label name: Negative; certainty: 0.916016\n",
      "Sentence:  It is not bad, but it could be better\n",
      "Label: 1; label name: Positive; certainty: 0.882812\n"
     ]
    }
   ],
   "source": [
    "class_dict={\n",
    "    1:\"Positive\",\n",
    "    2:\"Negative\"\n",
    "}\n",
    "\n",
    "sentence1 = [\"I hate when this situations happen\"]\n",
    "labels1 = classifier.predict_proba(sentence1)\n",
    "class1, prob1 = labels1[0][0] # it returns class as string\n",
    "print(\"Sentence: \", sentence1[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class1, class_dict[int(class1)], prob1))\n",
    "\n",
    "sentence2 = ['It is not bad, but it could be better']\n",
    "labels2 = classifier.predict_proba(sentence2)\n",
    "class2, prob2 = labels2[0][0] # it returns class as string\n",
    "print(\"Sentence: \", sentence2[0])\n",
    "print(\"Label: %s; label name: %s; certainty: %f\" %(class2, class_dict[int(class2)], prob2))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
