{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recommendation Systems\n",
    "\n",
    "In this tutorial we are going to use a [deep autoencoder](https://arxiv.org/abs/1708.01715) to perform collaborative filtering in the [Netflix dataset](https://netflixprize.com/). \n",
    "\n",
    "[Collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering) is one of the most pupular techniques in recommendation systems. It is based on inferring the missing entries in an `mxn` matrix, `R`, whose `(i, j)` entry describes the ratings given by the `ith` user to the `jth` item. The performance is then measured using Root\n",
    "Mean Squared Error (RMSE).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/5/52/Collaborative_filtering.gif\" width=300px/>\n",
    "</p>\n",
    "\n",
    "The code in this tutorial is done with [PyTorch](http://pytorch.org/) and is based on [this repo](https://github.com/NVIDIA/DeepRecommender) by NVIDIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.4 | packaged by conda-forge | (default, Nov  4 2017, 10:11:29) \n",
      "[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
      "PyTorch:  0.3.0.post4\n",
      "Numpy:  1.14.0\n",
      "Number of CPU processors:  24\n",
      "GPU:  ['Tesla M60', 'Tesla M60', 'Tesla M60', 'Tesla M60']\n",
      "GPU memory:  ['8123 MiB', '8123 MiB', '8123 MiB', '8123 MiB']\n",
      "CUDA:  CUDA Version 8.0.61\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from utils import get_gpu_name, get_number_processors, get_gpu_memory, get_cuda_version\n",
    "\n",
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"PyTorch: \", torch.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Number of CPU processors: \", get_number_processors())\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(\"GPU memory: \", get_gpu_memory())\n",
    "print(\"CUDA: \", get_cuda_version())\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Netflix\n",
    "\n",
    "This dataset was constructed to support participants in the [Netflix Prize](http://www.netflixprize.com). The movie rating files contain over 100 million ratings from 480 thousand randomly-chosen, anonymous Netflix customers over 17 thousand movie titles.  The data were collected between October, 1998 and December, 2005 and reflect the distribution of all ratings received during this period.  The ratings are on a scale from 1 to 5 (integral) stars.\n",
    "\n",
    "The dataset can be [downloaded here](http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a). To uncompress it:\n",
    "\n",
    "```bash\n",
    "tar -xvf nf_prize_dataset.tar.gz\n",
    "tar -xf download/training_set.tar\n",
    "```\n",
    "\n",
    "When we download the data, there are two important files:\n",
    "\n",
    "1) The file `training_set.tar` is a tar of a directory containing 17770 files, one per movie.  The first line of each file contains the movie id followed by a colon.  Each subsequent line in the file corresponds to a rating from a customer and its date in the following format:\n",
    "\n",
    "CustomerID,Rating,Date\n",
    "- MovieIDs range from 1 to 17770 sequentially.\n",
    "- CustomerIDs range from 1 to 2649429, with gaps. There are 480189 users.\n",
    "- Ratings are on a five star (integral) scale from 1 to 5.\n",
    "- Dates have the format YYYY-MM-DD.\n",
    "\n",
    "2) Movie information in `movie_titles.txt` is in the following format:\n",
    "\n",
    "MovieID,YearOfRelease,Title\n",
    "\n",
    "- MovieID do not correspond to actual Netflix movie ids or IMDB movie ids.\n",
    "- YearOfRelease can range from 1890 to 2005 and may correspond to the release of corresponding DVD, not necessarily its theaterical release.\n",
    "- Title in English is the Netflix movie.\n",
    "\n",
    "### Data prep\n",
    "\n",
    "The first step is to covert the data to the correct format for the autoencoder to read. This can take between 1 to 2 hours.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = '/datadrive'\n",
    "NF_PRIZE_DATASET = os.path.join(DATA_ROOT, 'netflix','download','training_set') #location of extracted data\n",
    "NF_DATA = 'Netflix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%run ./DeepRecommender/data_utils/netflix_data_convert.py $NF_PRIZE_DATASET $NF_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script splitted the data into train, test and validation set, creating files with three columns: `CustomerID,MovieID,Rating`. The data is splitted over time generating 4 datasets: Netflix 3months, Netflix 6 months, Netflix 1 year and Netflix full. Here there is a table with some details of each dataset:\n",
    "\n",
    "| Dataset  | Netflix 3 months | Netflix 6 months | Netflix 1 year | Netflix full |\n",
    "| -------- | ---------------- | ---------------- | ----------- |  ------------ |\n",
    "| Ratings train | 13,675,402 | 29,179,009 | 41,451,832 | 98,074,901 |\n",
    "| Users train | 311,315 |390,795  | 345,855 | 477,412 |\n",
    "| Items train | 17,736 |17,757  | 16,907 | 17,768 |\n",
    "| Time range train | 2005-09-01 to 2005-11-31 | 2005-06-01 to 2005-11-31 | 2004-06-01 to 2005-05-31 | 1999-12-01 to 2005-11-31\n",
    "|  |  |  |   | |\n",
    "| Ratings test | 2,082,559 | 2,175,535  | 3,888,684| 2,250,481 |\n",
    "| Users test | 160,906 | 169,541  | 197,951| 173,482 |\n",
    "| Items test | 17,261 | 17,290  | 16,506| 17,305 |\n",
    "| Time range test | 2005-12-01 to 2005-12-31 | 2005-12-01 to 2005-12-31 | 2005-06-01 to 2005-06-31 | 2005-12-01 to 2005-12-31\n",
    "\n",
    "Let's take a look at one of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1041739, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1549</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5144</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7716</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8348</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4635</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  MovieID  Rating\n",
       "0           0     1549     1.0\n",
       "1           0     5144     2.0\n",
       "2           0     7716     3.0\n",
       "3           0     8348     3.0\n",
       "4           0     4635     2.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf_3m_valid = os.path.join(NF_DATA, 'N3M_VALID', 'n3m.valid.txt')\n",
    "df = pd.read_csv(nf_3m_valid, names=['CustomerID','MovieID','Rating'], sep='\\t')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Autoencoder for Collaborative Filtering\n",
    "\n",
    "Once we have the data, let's explain in some detail the model that we are going to use. The [model](https://arxiv.org/abs/1708.01715) developed by NVIDIA folks is a Deep autoencoder with 6 layers with non-linear activation function SELU (scaled exponential linear units), dropout and iterative dense refeeding.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
